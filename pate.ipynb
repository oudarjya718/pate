{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHON6Ip3H3Fm",
        "colab_type": "text"
      },
      "source": [
        "summary :-In this blog we’re going to discuss PATE . PATE or \"Private Aggregation of Teacher Ensembles\" is a private machine learning technique that was created by Nicolas Papernot et. al. , published in ICLR 2018. In applications like in financial or medical applications performing machine learning involves sensitive data, and PATE is an approach to perform machine learning on this kind of sensitive data with different notions of privacy guarantees involved. In PATE we need to split the sensitive data into a certain no. of training sets and train a classifier on each of those sets. Then we need to use the classzifiers to predict the labels of the public data. \n",
        "\n",
        "![picture](https://drive.google.com/file/d/1tipF9kmxCfJk7X_Hkaen5MH0BuNZfYQG/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtVIrA33Xbce",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Lets discuss some challenges of performing machine Learning on Private data, here are some attacks that have been seen in the security literature :- \n",
        "\n",
        "1) Training data extraction attacks or model inversion attacks :- Imagine that you have a classifier trained on images of faces of individuals, to recognize which individual is in the image, e.g. you feed the classifier some face images and the the output will classify the person in the image. Frederikson et. al. showed that with access to the classifier’s output probabilities they were able to reconstruct these images here which are approximations of the training data that the machine learning classifier saw. These approximations are not extracting individual training points but rather the average representations that the classifier learned for each class which here corresponds to one individual.\n",
        "\n",
        "2) Membership attacks :- The second kind of attack is membership inference attacks against ML models introduced by by Shokri et. al.. Here the goal of the adversary is slightly different, instead of reconstructing training points from the output of the classifier the goal is now to infer whether a specific input was used to train the model, given the image of a person, did the person contribute to the training data from a specific machine learning model. What Shokri et.al. also showed is that you can perform these attacks by only having access to the classifier’s probabilities.\n",
        "\n",
        "![picture](https://drive.google.com/file/d/1qJUHJkbQApgVWBE0SYRCHpOA4owiokO_/view?usp=sharing)\n",
        "\n",
        "PATE  analysis is a way to defend these data holders against these powerful attackers, generally considering two types of threat models i.e. black box and white box adversaries :- \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKufMB0VXloE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1) Model querying(black-box-adversary) :-  In this kind of attack the adversary is only able to query the model that you trained, it will not have access to the internals of the model, to the architectures to the parameters. All it can do is submit inputs to your black-box model and observe the prediction that the model is making. This is called model querying attacks or black box attacks , the two attacks presented above are instances of such attacks.(Shokri et.al. & Frederikson et.al.)\n",
        "\n",
        "2) Model inspection(white-box adversary) :-  This kind of attack is stronger as in this case the adversary has access to the model and its parameters.  The work by Zhang et.al. i.e [Understanding Deep Learning requires re-thinking generalization](https://arxiv.org/pdf/1611.03530.pdf) kind of hints at the fact that machine learning models might be able to memorize some of their training data or at least they have the capacity to do so. So we need to be robust to an attacker that has access to these model parameters and can analyze them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3BhkwWMH2-T",
        "colab_type": "text"
      },
      "source": [
        "This is why while working on PATE analysis the threat models considered very powerful adversaries, which can make a potentially unbounded number of queries and also have access to the model  internals.\n",
        "\n",
        "The way it is proved that PATE provides differential privacy is based on an application called \"Moments Accountant\" which was a technique introduced by Abadi et. al. in 2016 [paper](https://arxiv.org/pdf/1607.00133.pdf). The Moments accountant technique basically allows us to formalize the fact that when we have a strong quorum among the teachers when all of the teachers or most of the teachers agree then we should pay a small privacy cost for that prediction given to the student. The guarantees that we provide here in terms of differential privacy are \"data dependant\", which means that during training all the votes provided by the teachers are recorded and we compute numerical values for the differential privacy guarantees provided. So here in a sense differential privacy is characterized by two values epsilon and delta.\n",
        "\n",
        "![picture](https://drive.google.com/file/d/16BR8daoRkzMECGi1c-_l6WS8Ur50fyvM/view?usp=sharing)\n",
        "\n",
        "Epsilon or the privacy budget is the value which basically defines an interval, which quantifies how much we tolerate the output of the machine learning model on one database to be different from the output of the same machine learning model on second database D prime that only has one training point that is different. The smaller the epsilon is , the stronger the privacy will be.\n",
        "\n",
        "Delta is the failure rate which we tolerate the guarantee to not hold.\n",
        "\n",
        "https://gyazo.com/2ec189622a995e5da4e530e9c80dfc72\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnXSGhWORV9T",
        "colab_type": "text"
      },
      "source": [
        "**The first thing we need to do before getting started is install syft**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZBw5u1QRUoQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e29eebb-d331-4149-b6d1-d210e160f3c5"
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: syft in /usr/local/lib/python3.6/dist-packages (0.2.8)\n",
            "Requirement already satisfied: lz4~=3.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (3.0.2)\n",
            "Requirement already satisfied: Pillow~=6.2.2 in /usr/local/lib/python3.6/dist-packages (from syft) (6.2.2)\n",
            "Requirement already satisfied: aiortc==0.9.28 in /usr/local/lib/python3.6/dist-packages (from syft) (0.9.28)\n",
            "Requirement already satisfied: flask-socketio~=4.2.1 in /usr/local/lib/python3.6/dist-packages (from syft) (4.2.1)\n",
            "Requirement already satisfied: torchvision~=0.5.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.5.0)\n",
            "Requirement already satisfied: tornado==4.5.3 in /usr/local/lib/python3.6/dist-packages (from syft) (4.5.3)\n",
            "Requirement already satisfied: dill~=0.3.1 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.2)\n",
            "Requirement already satisfied: websocket-client~=0.57.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.57.0)\n",
            "Requirement already satisfied: notebook==5.7.8 in /usr/local/lib/python3.6/dist-packages (from syft) (5.7.8)\n",
            "Requirement already satisfied: psutil==5.7.0 in /usr/local/lib/python3.6/dist-packages (from syft) (5.7.0)\n",
            "Requirement already satisfied: numpy~=1.18.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.18.5)\n",
            "Requirement already satisfied: importlib-resources~=1.5.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.5.0)\n",
            "Requirement already satisfied: Flask~=1.1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.2)\n",
            "Requirement already satisfied: msgpack~=1.0.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.0.0)\n",
            "Requirement already satisfied: torch~=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: phe~=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: openmined.threepio==0.2.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.2.0)\n",
            "Requirement already satisfied: RestrictedPython~=5.0 in /usr/local/lib/python3.6/dist-packages (from syft) (5.0)\n",
            "Requirement already satisfied: syft-proto~=0.5.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.5.1)\n",
            "Requirement already satisfied: scipy~=1.4.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.1)\n",
            "Requirement already satisfied: requests-toolbelt==0.9.1 in /usr/local/lib/python3.6/dist-packages (from syft) (0.9.1)\n",
            "Requirement already satisfied: tblib~=1.6.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.6.0)\n",
            "Requirement already satisfied: websockets~=8.1.0 in /usr/local/lib/python3.6/dist-packages (from syft) (8.1)\n",
            "Requirement already satisfied: requests~=2.22.0 in /usr/local/lib/python3.6/dist-packages (from syft) (2.22.0)\n",
            "Requirement already satisfied: pylibsrtp>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from aiortc==0.9.28->syft) (0.6.6)\n",
            "Requirement already satisfied: cryptography>=2.2 in /usr/local/lib/python3.6/dist-packages (from aiortc==0.9.28->syft) (3.0)\n",
            "Requirement already satisfied: pyee>=6.0.0 in /usr/local/lib/python3.6/dist-packages (from aiortc==0.9.28->syft) (7.0.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiortc==0.9.28->syft) (0.7)\n",
            "Requirement already satisfied: av<9.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from aiortc==0.9.28->syft) (8.0.2)\n",
            "Requirement already satisfied: crc32c in /usr/local/lib/python3.6/dist-packages (from aiortc==0.9.28->syft) (2.0.1)\n",
            "Requirement already satisfied: aioice<0.7.0,>=0.6.17 in /usr/local/lib/python3.6/dist-packages (from aiortc==0.9.28->syft) (0.6.18)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from aiortc==0.9.28->syft) (1.14.1)\n",
            "Requirement already satisfied: python-socketio>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from flask-socketio~=4.2.1->syft) (4.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision~=0.5.0->syft) (1.15.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook==5.7.8->syft) (5.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook==5.7.8->syft) (2.11.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook==5.7.8->syft) (5.6.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.6/dist-packages (from notebook==5.7.8->syft) (19.0.2)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from notebook==5.7.8->syft) (5.3.5)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook==5.7.8->syft) (4.10.1)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook==5.7.8->syft) (4.3.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook==5.7.8->syft) (0.8.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.6/dist-packages (from notebook==5.7.8->syft) (0.8.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook==5.7.8->syft) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from notebook==5.7.8->syft) (4.6.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook==5.7.8->syft) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources~=1.5.0->syft) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources~=1.5.0->syft) (3.1.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from RestrictedPython~=5.0->syft) (49.2.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.6/dist-packages (from syft-proto~=0.5.0->syft) (3.12.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (1.24.3)\n",
            "Requirement already satisfied: netifaces in /usr/local/lib/python3.6/dist-packages (from aioice<0.7.0,>=0.6.17->aiortc==0.9.28->syft) (0.10.9)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0.0->aiortc==0.9.28->syft) (2.20)\n",
            "Requirement already satisfied: python-engineio>=3.13.0 in /usr/local/lib/python3.6/dist-packages (from python-socketio>=4.3.0->flask-socketio~=4.2.1->syft) (3.13.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook==5.7.8->syft) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook==5.7.8->syft) (1.1.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook==5.7.8->syft) (0.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook==5.7.8->syft) (0.6.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook==5.7.8->syft) (2.1.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook==5.7.8->syft) (3.1.5)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook==5.7.8->syft) (0.4.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook==5.7.8->syft) (1.4.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook==5.7.8->syft) (0.8.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.0->notebook==5.7.8->syft) (2.8.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->notebook==5.7.8->syft) (5.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook==5.7.8->syft) (4.4.2)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook==5.7.8->syft) (0.6.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook==5.7.8->syft) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook==5.7.8->syft) (20.4)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook==5.7.8->syft) (0.7.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert->notebook==5.7.8->syft) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook==5.7.8->syft) (0.2.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir5dBAhcJK4G",
        "colab_type": "text"
      },
      "source": [
        "**Load the SVHN data set**\n",
        "\n",
        "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.italicized text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdKYwY0mF_ny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "654dcb4e-cde1-49d1-dc4f-e7f42410df37"
      },
      "source": [
        "import torch\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "train_data = datasets.SVHN('datasets/SVHN/train/', split='train', transform=transform,\n",
        "                                 target_transform=None, download=True)\n",
        "test_data = datasets.SVHN('datasets/SVHN/test/', split='test', transform=transform,\n",
        "                               target_transform=None, download=True)\n",
        "\n",
        "num_teachers = 100 \n",
        "batch_size = 50 \n",
        "\n",
        "def get_data_loaders(train_data, num_teachers):\n",
        "    \"\"\" Function to create data loaders for the Teacher classifier \"\"\"\n",
        "    teacher_loaders = []\n",
        "    data_size = len(train_data) // num_teachers\n",
        "    \n",
        "    for i in range(data_size):\n",
        "        indices = list(range(i*data_size, (i+1)*data_size))\n",
        "        subset_data = Subset(train_data, indices)\n",
        "        loader = torch.utils.data.DataLoader(subset_data, batch_size=batch_size)\n",
        "        teacher_loaders.append(loader)\n",
        "        \n",
        "    return teacher_loaders\n",
        "\n",
        "teacher_loaders = get_data_loaders(train_data, num_teachers)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: datasets/SVHN/train/train_32x32.mat\n",
            "Using downloaded and verified file: datasets/SVHN/test/test_32x32.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-o0agNFGVXp",
        "colab_type": "text"
      },
      "source": [
        "**Generating the student train and test data by splitting the svhn test set**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg52nf-9GVQO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "**Using 90% of the test data as train data and the remaining 10% as test data and creating the public data set**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2S6bg2VKcgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "student_train_data = Subset(test_data, list(range(9000)))\n",
        "student_test_data = Subset(test_data, list(range(9000, 10000)))\n",
        "\n",
        "student_train_loader = torch.utils.data.DataLoader(student_train_data, batch_size=batch_size)\n",
        "student_test_loader = torch.utils.data.DataLoader(student_test_data, batch_size=batch_size)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apTVHL9iblj-",
        "colab_type": "text"
      },
      "source": [
        "**Define the teacher models and train them by defining a cnn**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cUI46XjWc2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(5*10*10, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(x.size(0), 5*10*10)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N54--pfbbvR-",
        "colab_type": "text"
      },
      "source": [
        "**Defining the train and predict functions**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-PI0Eh1WiAg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7bde4e10-dd6c-4a3e-965c-2d77a1639d61"
      },
      "source": [
        "def train(model, trainloader, criterion, optimizer, epochs=10):\n",
        "    running_loss = 0\n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "              \n",
        "        for images, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model.forward(images)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "def predict(model, dataloader):\n",
        "    outputs = torch.zeros(0, dtype=torch.long)\n",
        "    model.eval()\n",
        "    \n",
        "    for images, labels in dataloader:\n",
        "        output = model.forward(images)\n",
        "        ps = torch.argmax(torch.exp(output), dim=1)\n",
        "        outputs = torch.cat((outputs, ps))\n",
        "        \n",
        "    return outputs\n",
        "def train_models(num_teachers):\n",
        "    models = []\n",
        "    for i in range(num_teachers):\n",
        "        model = Classifier()\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "        train(model, teacher_loaders[i], criterion, optimizer)\n",
        "        models.append(model)\n",
        "    return models\n",
        "models = train_models(num_teachers)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdEFdMK4b13m",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "**Next by combining the predictions of the Teacher models we will generate the Aggregated Teacher and Student labels.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzsAfpVYWmPY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f4fee706-0142-4f7a-b357-8ee8b5c7d02f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "epsilon = 0.2\n",
        "def aggregated_teacher(models, dataloader, epsilon):\n",
        "\n",
        "    preds = torch.torch.zeros((len(models), 9000), dtype=torch.long)\n",
        "    for i, model in enumerate(models):\n",
        "        results = predict(model, dataloader)\n",
        "        preds[i] = results\n",
        "    \n",
        "    labels = np.array([]).astype(int)\n",
        "    for image_preds in np.transpose(preds):\n",
        "        label_counts = np.bincount(image_preds, minlength=10)\n",
        "        beta = 1 / epsilon\n",
        "\n",
        "        for i in range(len(label_counts)):\n",
        "            label_counts[i] += np.random.laplace(0, beta, 1)\n",
        "\n",
        "        new_label = np.argmax(label_counts)\n",
        "        labels = np.append(labels, new_label)\n",
        "    \n",
        "    return preds.numpy(), labels\n",
        "teacher_models = models\n",
        "preds, student_labels = aggregated_teacher(teacher_models, student_train_loader, epsilon)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ2hlVJWchle",
        "colab_type": "text"
      },
      "source": [
        "**Now by using the labels generated previously we will create the Student model and train it**\n",
        "\n",
        "Why do we need to train an additional student model ?\n",
        "The aggregated teacher violates our threat model :- \n",
        "\n",
        "1.The total privacy loss gets increased by each prediction- Privacy budgets create a tension between the acuracy and number of predictions. (If we stick to the first part of the mechanism where we have only the teachers everytime the teachers make prediction we pay an additional cost in privacy.  So overall as users make more and more prediction queries the overall cost in terms of privacy will keep increasing so at some point we will have a tension between utility and privacy)\n",
        "\n",
        "2.Pivate data may get revealed by the inspection of internals  - Privacy guarantees should hold in the face of white-box adversaries.( If you remember a threat model we considered adversaries are able to access the internasl of the models which means that if the adversaries are able to inspect the internals of the teachers bcause the teachers saw the training data they may be able to leak some information about that training data whereas if the adversary instead inspects the student model the it will only be able recover in the worst case the public data with the labels that were provided by the teachers with differential privay. )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRQS7uy8Y5HU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "de215ff1-1d99-4235-b5f1-566bb219efab"
      },
      "source": [
        "def student_loader(student_train_loader, labels):\n",
        "    for i, (data, _) in enumerate(iter(student_train_loader)):\n",
        "        yield data, torch.from_numpy(labels[i*len(data): (i+1)*len(data)])\n",
        "student_model = Classifier()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.003)\n",
        "epochs = 10\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "for e in range(epochs):\n",
        "    student_model.train()\n",
        "    train_loader = student_loader(student_train_loader, student_labels)\n",
        "    for images, labels in train_loader:\n",
        "        steps += 1\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = student_model.forward(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if steps % 50 == 0:\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            student_model.eval()\n",
        "            with torch.no_grad():\n",
        "                for images, labels in student_test_loader:\n",
        "                    log_ps = student_model(images)\n",
        "                    test_loss += criterion(log_ps, labels).item()\n",
        "                    \n",
        "                    ps = torch.exp(log_ps)\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "            student_model.train()\n",
        "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Train Loss: {:.3f}.. \".format(running_loss/len(student_train_loader)),\n",
        "                  \"Test Loss: {:.3f}.. \".format(test_loss/len(student_test_loader)),\n",
        "                  \"Accuracy: {:.3f}\".format(accuracy/len(student_test_loader)))\n",
        "            running_loss = 0\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10..  Train Loss: 0.321..  Test Loss: 3.638..  Accuracy: 0.217\n",
            "Epoch: 1/10..  Train Loss: 0.254..  Test Loss: 5.541..  Accuracy: 0.217\n",
            "Epoch: 1/10..  Train Loss: 0.244..  Test Loss: 5.642..  Accuracy: 0.217\n",
            "Epoch: 2/10..  Train Loss: 0.223..  Test Loss: 5.961..  Accuracy: 0.217\n",
            "Epoch: 2/10..  Train Loss: 0.205..  Test Loss: 6.295..  Accuracy: 0.254\n",
            "Epoch: 2/10..  Train Loss: 0.186..  Test Loss: 6.514..  Accuracy: 0.303\n",
            "Epoch: 2/10..  Train Loss: 0.171..  Test Loss: 5.364..  Accuracy: 0.296\n",
            "Epoch: 3/10..  Train Loss: 0.170..  Test Loss: 7.715..  Accuracy: 0.306\n",
            "Epoch: 3/10..  Train Loss: 0.162..  Test Loss: 6.738..  Accuracy: 0.317\n",
            "Epoch: 3/10..  Train Loss: 0.154..  Test Loss: 7.329..  Accuracy: 0.321\n",
            "Epoch: 4/10..  Train Loss: 0.160..  Test Loss: 5.704..  Accuracy: 0.328\n",
            "Epoch: 4/10..  Train Loss: 0.140..  Test Loss: 7.554..  Accuracy: 0.326\n",
            "Epoch: 4/10..  Train Loss: 0.144..  Test Loss: 6.204..  Accuracy: 0.347\n",
            "Epoch: 4/10..  Train Loss: 0.138..  Test Loss: 9.186..  Accuracy: 0.320\n",
            "Epoch: 5/10..  Train Loss: 0.139..  Test Loss: 7.217..  Accuracy: 0.343\n",
            "Epoch: 5/10..  Train Loss: 0.141..  Test Loss: 6.633..  Accuracy: 0.338\n",
            "Epoch: 5/10..  Train Loss: 0.133..  Test Loss: 7.037..  Accuracy: 0.356\n",
            "Epoch: 5/10..  Train Loss: 0.131..  Test Loss: 8.393..  Accuracy: 0.342\n",
            "Epoch: 6/10..  Train Loss: 0.132..  Test Loss: 7.622..  Accuracy: 0.349\n",
            "Epoch: 6/10..  Train Loss: 0.132..  Test Loss: 8.412..  Accuracy: 0.341\n",
            "Epoch: 6/10..  Train Loss: 0.131..  Test Loss: 7.899..  Accuracy: 0.332\n",
            "Epoch: 7/10..  Train Loss: 0.134..  Test Loss: 8.143..  Accuracy: 0.343\n",
            "Epoch: 7/10..  Train Loss: 0.132..  Test Loss: 7.602..  Accuracy: 0.330\n",
            "Epoch: 7/10..  Train Loss: 0.133..  Test Loss: 8.767..  Accuracy: 0.341\n",
            "Epoch: 7/10..  Train Loss: 0.121..  Test Loss: 7.915..  Accuracy: 0.345\n",
            "Epoch: 8/10..  Train Loss: 0.129..  Test Loss: 7.880..  Accuracy: 0.334\n",
            "Epoch: 8/10..  Train Loss: 0.129..  Test Loss: 8.370..  Accuracy: 0.349\n",
            "Epoch: 8/10..  Train Loss: 0.129..  Test Loss: 8.286..  Accuracy: 0.357\n",
            "Epoch: 9/10..  Train Loss: 0.130..  Test Loss: 8.182..  Accuracy: 0.339\n",
            "Epoch: 9/10..  Train Loss: 0.124..  Test Loss: 9.138..  Accuracy: 0.347\n",
            "Epoch: 9/10..  Train Loss: 0.129..  Test Loss: 8.023..  Accuracy: 0.354\n",
            "Epoch: 9/10..  Train Loss: 0.126..  Test Loss: 8.028..  Accuracy: 0.337\n",
            "Epoch: 10/10..  Train Loss: 0.124..  Test Loss: 8.121..  Accuracy: 0.354\n",
            "Epoch: 10/10..  Train Loss: 0.129..  Test Loss: 7.686..  Accuracy: 0.347\n",
            "Epoch: 10/10..  Train Loss: 0.122..  Test Loss: 7.507..  Accuracy: 0.362\n",
            "Epoch: 10/10..  Train Loss: 0.117..  Test Loss: 8.355..  Accuracy: 0.357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxtiJ0-bcpem",
        "colab_type": "text"
      },
      "source": [
        "**Now we will perform PATE Analysis on the student labels generated by the Aggregated Teacher**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsDWmx5ydWSH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4054e373-e2eb-4c06-cee3-8986f5142ae0"
      },
      "source": [
        "from syft.frameworks.torch.dp import pate\n",
        "\n",
        "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=student_labels, noise_eps=epsilon, delta=1e-5)\n",
        "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
        "print(\"Data Dependent Epsilon:\", data_dep_eps)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Independent Epsilon: 1451.5129254649705\n",
            "Data Dependent Epsilon: 59.47392676433782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgzmpkcUd3iB",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion :-** \n",
        "\n",
        "While performing different Differential privacy techniques it is important to note which technique to use in the given scenario. PATE is generally useful when a party wants to annotate a local data set using the private data sets of other actors, and the Epsilon-delta tool allows for very granular control of just how much the other actors must trust us to protect their privacy in this process. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjDIVRc1d3lD",
        "colab_type": "text"
      },
      "source": [
        "Credits :- \n",
        "\n",
        "[Nicolas Papernot - Private Machine Learning with PATE - Cybersecurity With The Best 2017](https://www.youtube.com/watch?v=cjo_u_yT2wQ&t=687s)\n",
        "\n",
        "\n",
        "[SCALABLE PRIVATE LEARNING WITH PATE paper](https://arxiv.org/pdf/1802.08908.pdf)\n",
        "\n",
        "[SEMI-SUPERVISED KNOWLEDGE TRANSFER\n",
        "FOR DEEP LEARNING FROM PRIVATE TRAINING DATA paper](https://arxiv.org/pdf/1610.05755.pdf)\n",
        "\n",
        "[A 5-Step Guide on incorporating Differential Privacy into your Deep Learning models\n",
        "](https://towardsdatascience.com/a-5-step-guide-on-incorporating-differential-privacy-into-your-deep-learning-models-7861c6c822c4)\n",
        "\n",
        "[Secure and Private-AI course](https://www.udacity.com/course/secure-and-private-ai--ud185)\n",
        "\n",
        "[Nicolas Papernot and Ian goodfellow Clever-Hans blog](http://www.cleverhans.io/privacy/2018/04/29/privacy-and-machine-learning.html)\n",
        "\n",
        "[MAINTAINING PRIVACY IN MEDICAL DATA WITH DIFFERENTIAL PRIVACY\n",
        "](https://blog.openmined.org/maintaining-privacy-in-medical-data-with-differential-privacy/)\n",
        "\n",
        "[UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION\n",
        "](https://arxiv.org/pdf/1611.03530.pdf)\n",
        "\n",
        "[Membership Inference Attacks Against\n",
        "Machine Learning Models](https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf)\n",
        "\n",
        "[SVHN Website](http://ufldl.stanford.edu/housenumbers/#:~:text=SVHN%20is%20a%20real-world,on%20data%20preprocessing%20and%20formatting.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRzPrt-5d3gE",
        "colab_type": "text"
      },
      "source": [
        "If you want to join our mission on making the world more privacy preserving\n",
        "\n",
        "[Join OpenMined slack](http://slack.openmined.org/)\n",
        "\n",
        "[Check OpenMined's GitHub](https://github.com/OpenMined)\n",
        "\n",
        "[OpenMined Welcome Package](https://github.com/OpenMined/OM-Welcome-Package)\n",
        "\n",
        "[Placements at OpenMined](https://placements.openmined.org/)\n",
        "\n",
        "[]()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nQN9gHQePEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}